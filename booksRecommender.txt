extract data from the datasets available on kaggle into cache using boiler plate provided by kaggle
extract the csv into the pandas.dataframe to perform data cleaning:
    use dataWrangler extention to provide overall data discription
    look for the missing values in the data columns using their truth values and also observe the relativity in the missing
        visualize the data using seaborn and matplotlib.pyplot
    we can see the rating and pages are relativity missing for some books and
        without discription we can't recommend and book to someone.
    check the corelation b/w these columns if they are near to 1 or -1 using spearmen method
        then are directly or indirectly related else not related
    in our case they are not related to each other so remove the rows that contain null values in any of these columns
        by using the masking of | truth values of these columns and remove them from the og books
    data consist of many categories which are not real so plot a count graph based on those categories
        this is resolved using LLM Algorithms
    the data which has less discriptionWords is useless so remove those
    group the title and subtitle into new col using the numpy.where()
    drop the unnessasary columns and export the df to csv
theoritical knowledge about LLM's:
    most of the LLM like gpt, claude, deepseek are trained through transformer model architecture
    the transformer consist of encoder and decoder blocks but the LLM like gpt consist of only decoder
    the text is broken down to tokens and tokens are grouped based on their weights similarity with other tokens and aswell as itself
    at first the grouping of tokens based on their weights are inappropriate but through lots of training and testing
        the weights in a group are made as close as possible through vector embeddings anf self attention mechanism
    this thing for source text happens in the encoder block and for target text happens in the decoder block
        when the refined self attention vetors from encoder are fed into decoder it predicts the word by word
        translation for the source text based on decoder's refined self attention vetors
    the input text is converted in self attention vectors and compared with already existing vector embeddings
        and similarity score is generated based on different similarity mechanism
    the self attention vectors are assigned similar id's in encoder aswell decoder for easy metadata retrival
Now we need to recommend the books based on their query relating to the Description using OpenAi's openApi:
    we need to separate the taggedDescription into a file for linking b/w Description and Og Books.csv
    we need langchain's TextReader: to read from a .txt to readable format for langchain,
        CharacterTextSplitter: to split the taggedDescription
    we use OpenAiEmbedding to Assign weights for each of the tokens in the taggedDescription
        and grouping them based on their weights similarity using encoder Blocks
    After that we assign them to the vector Database: Chroma based on the Self attention vectors
        for easy performing of CRED ops on the Database
    the OpenAiEmbedding perform the similarity search based on the query provided
we classify the board range of categories into Narrow range:
    since the top_k categories hold most % of the data we categorise them into 2 main categories fiction and non-fiction
    then we use huggingFace text classification model to classify the top_k model into fiction and Non-fiction
    then we get the accuracy of the model using the predictedCategories and actually assignedCategories
        since it is a zero-shot model the accuracy is not at peeks
    then we classify the rest: n-top-k categories based on this model and put them into a .csv file
we assign emotions categories to the books based on their Description:
    we use the same huggingFace text classification model but this time we use specific to the emotions
    since the Description is lengthy we split and assign what percent of emotions each Description part has to offer
        then we take the mean of it
    and Append it to the og books.csv
we use Gradio framework which provides acts a platform to display our ML projects:
    we specify the data/info to be displayed in the webpage and specify the dropdown's etc and launch the dashboard

to run the application in local:
    clone the gitHub repo: git clone https://github.com/Ranjith-singh/BookSenseAI.git
    run the file using: python GradioBasedDashboard.py
